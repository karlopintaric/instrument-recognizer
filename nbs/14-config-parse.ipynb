{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import transforms as transform_module\n",
    "import yaml\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(train_dir='./data/raw/IRMAS_Training_Data', valid_dir='./data/raw/IRMAS_Validation_Data', preprocess={'PreprocessPipeline': {'target_sr': 22050}}, transforms={'Preemphasis': None, 'Spectrogram': {'sample_rate': 22050, 'n_mels': 128, 'hop_length': 160, 'n_fft': 800}, 'LogTransform': None, 'PadCutToLength': {'max_length': 1024}, 'Normalize': {'mean': -4.2677393, 'std': 9.1379948}}, signal_augments=None, spec_augments=None, batch_size=8, model_name='MIT/ast-finetuned-audioset-10-10-0.4593', loss={'FocalLoss': {'alpha': 0.25, 'gamma': 2}}, optimizer={'AdamW': {'weight_decay': 0.0001}}, learning_rates={'base_model': {'lr': 1e-08}, 'classifier': {'lr': 0.001}}, num_accum=2, verbose=True, EPOCHS=4, metrics=['hamming_score', 'zero_one_score', 'mean_average_precision', 'mean_f1_score'], scheduler={'OneCycleLR': {'pct_start': 0.3, 'anneal_strategy': 'cos'}}, early_stopping={'patience': 3, 'min_delta': 0.0001}, save_best_model=True)\n"
     ]
    }
   ],
   "source": [
    "CONFIG_PATH = \"../config.yaml\"\n",
    "\n",
    "with open(CONFIG_PATH) as file:\n",
    "  config = SimpleNamespace(**yaml.safe_load(file))\n",
    "  \n",
    "  print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_objs(fn_dict, module):\n",
    "    \n",
    "    if fn_dict is None:\n",
    "        return None\n",
    "\n",
    "    transforms = []\n",
    "    for transform in fn_dict.keys():\n",
    "        fn = getattr(module, transform)\n",
    "        fn_args = fn_dict[transform]\n",
    "        \n",
    "        if fn_args is None:\n",
    "            transforms.append(fn())\n",
    "        else:\n",
    "            transforms.append(fn(**fn_args))\n",
    "    \n",
    "    return transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "transforms = []\n",
    "fn_dict = config.preprocess\n",
    "for transform in fn_dict.keys():\n",
    "    fn = getattr(transform_module, transform)\n",
    "    fn_args = fn_dict[transform]\n",
    "    \n",
    "    transforms.append(fn(**fn_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(train_dir='./data/raw/IRMAS_Training_Data',\n",
       "          valid_dir='./data/raw/IRMAS_Validation_Data',\n",
       "          preprocess={'PreprocessPipeline': {'target_sr': 22050}},\n",
       "          transforms={'Preemphasis': None,\n",
       "                      'Spectrogram': {'sample_rate': 22050,\n",
       "                       'n_mels': 128,\n",
       "                       'hop_length': 160,\n",
       "                       'n_fft': 800},\n",
       "                      'LogTransform': None,\n",
       "                      'PadCutToLength': {'max_length': 1024},\n",
       "                      'Normalize': {'mean': -4.2677393, 'std': 9.1379948}},\n",
       "          signal_augments=None,\n",
       "          spec_augments=None,\n",
       "          batch_size=8,\n",
       "          model_name='MIT/ast-finetuned-audioset-10-10-0.4593',\n",
       "          loss={'FocalLoss': {'alpha': 0.25, 'gamma': 2}},\n",
       "          optimizer={'AdamW': {'weight_decay': 0.0001}},\n",
       "          learning_rates={'base_model': {'lr': 1e-08},\n",
       "                          'classifier': {'lr': 0.001}},\n",
       "          num_accum=2,\n",
       "          verbose=True,\n",
       "          EPOCHS=4,\n",
       "          metrics=['hamming_score',\n",
       "                   'zero_one_score',\n",
       "                   'mean_average_precision',\n",
       "                   'mean_f1_score'],\n",
       "          scheduler={'OneCycleLR': {'pct_start': 0.3,\n",
       "                      'anneal_strategy': 'cos'}},\n",
       "          early_stopping={'patience': 3, 'min_delta': 0.0001},\n",
       "          save_best_model=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk-pintaric\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kpintaric/lumen-irmas/nbs/wandb/run-20230322_144550-6pulxo09</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/k-pintaric/lumen-irmas-nbs/runs/6pulxo09' target=\"_blank\">warm-sun-4</a></strong> to <a href='https://wandb.ai/k-pintaric/lumen-irmas-nbs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/k-pintaric/lumen-irmas-nbs' target=\"_blank\">https://wandb.ai/k-pintaric/lumen-irmas-nbs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/k-pintaric/lumen-irmas-nbs/runs/6pulxo09' target=\"_blank\">https://wandb.ai/k-pintaric/lumen-irmas-nbs/runs/6pulxo09</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace(**wandb.config.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(train_dir={'value': './data/processed/all_sync/IRMAS_Training_Data'},\n",
       "          valid_dir={'value': './data/raw/IRMAS_Validation_Data'},\n",
       "          preprocess={'PreprocessPipeline': {'parameters': {'target_sr': {'value': 22050}}}},\n",
       "          transforms={'Preemphasis': {'value': None},\n",
       "                      'Spectrogram': {'parameters': {'sample_rate': {'value': 22050},\n",
       "                        'n_mels': {'values': [128, 96, 64]},\n",
       "                        'hop_length': {'values': [64, 128, 256, 512]},\n",
       "                        'n_fft': {'values': [512, 1024, 2048]}}},\n",
       "                      'LogTransform': {'value': None},\n",
       "                      'PadCutToLength': {'parameters': {'max_length': {'value': 1024}}},\n",
       "                      'Normalize': {'parameters': {'mean': {'value': -4.2677393},\n",
       "                        'std': {'value': 9.1379948}}}},\n",
       "          batch_size={'value': 8},\n",
       "          loss={'FocalLoss': {'parameters': {'alpha': {'values': [-1,\n",
       "                    0.25,\n",
       "                    0.5,\n",
       "                    0.75]},\n",
       "                  'gamma': {'values': [1, 2, 3]}}}},\n",
       "          optimizer={'AdamW': {'parameters': {'weight_decay': {'distribution': 'log_uniform_values',\n",
       "                        'min': 1e-07,\n",
       "                        'max': 0.1}}}},\n",
       "          learning_rates.base_model={'parameters': {'lr': {'distribution': 'log_uniform_values',\n",
       "                                       'min': 1e-08,\n",
       "                                       'max': 1e-05}}},\n",
       "          learning_rates.classifier={'parameters': {'lr': {'distribution': 'log_uniform_values',\n",
       "                                       'min': 0.0001,\n",
       "                                       'max': 0.1}}},\n",
       "          num_accum={'values': [4, 6, 8]},\n",
       "          EPOCHS={'distribution': 'int_uniform', 'min': 1, 'max': 7},\n",
       "          scheduler={'OneCycleLR': {'parameters': {'pct_start': {'distribution': 'uniform',\n",
       "                        'min': 0,\n",
       "                        'max': 0.5},\n",
       "                       'anneal_strategy': {'value': 'cos'}}}},\n",
       "          early_stopping={'parameters': {'patience': {'value': 3},\n",
       "                           'min_delta': {'value': 0.0001}}},\n",
       "          save_best_model={'value': True},\n",
       "          verbose={'value': True},\n",
       "          metrics={'value': ['hamming_score',\n",
       "                    'zero_one_score',\n",
       "                    'mean_average_precision',\n",
       "                    'mean_f1_score']})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_model': {'parameters': {'lr': {'distribution': 'log_uniform_values',\n",
       "    'min': 1e-08,\n",
       "    'max': 1e-05}}},\n",
       " 'classifier': {'parameters': {'lr': {'distribution': 'log_uniform_values',\n",
       "    'min': 0.0001,\n",
       "    'max': 0.1}}}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.learning_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PreprocessPipeline': {'parameters': {'target_sr': {'value': 22050}}}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_obj(fn_dict, module, *args, **kwargs):\n",
    "    \n",
    "    if isinstance(fn_dict, dict):\n",
    "        fn_name = list(fn_dict.keys())[0]\n",
    "    if isinstance(fn_dict, str):\n",
    "        fn_name = fn_dict\n",
    "    \n",
    "    fn = getattr(module, fn_name)\n",
    "    fn_args = fn_dict[fn_name].get(\"parameters\")\n",
    "    \n",
    "    if fn_args is not None:\n",
    "        assert all([k not in fn_args for k in kwargs])\n",
    "        fn_args.update(kwargs)\n",
    "        \n",
    "        return fn(*args, **fn_args)\n",
    "    else:\n",
    "        return fn(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_transforms(config, module_name):\n",
    "    \n",
    "    if config is None:\n",
    "        return None\n",
    "\n",
    "    transforms = []\n",
    "    for transform in config.keys():\n",
    "        print(transform)\n",
    "        fn = getattr(transform_module, transform)\n",
    "        transforms.append(fn(config[transform][\"parameters\"]))\n",
    "    \n",
    "    return Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_objs(fn_dict, module):\n",
    "    \n",
    "    if fn_dict is None:\n",
    "        return None\n",
    "\n",
    "    transforms = []\n",
    "    for transform in fn_dict.keys():\n",
    "        fn = getattr(module, transform)\n",
    "        fn_args = fn_dict[transform]\n",
    "        \n",
    "        if fn_args is None:\n",
    "            transforms.append(fn())\n",
    "        else:\n",
    "            fn_args = fn_args.get(\"parameters\")\n",
    "            transforms.append(fn(**fn_args))\n",
    "    \n",
    "    return transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_dict = config.preprocess\n",
    "\n",
    "transforms = []\n",
    "for transform in fn_dict.keys():\n",
    "    fn = getattr(transform_module, transform)\n",
    "    fn_args = fn_dict[transform]\n",
    "\n",
    "    if fn_args is None:\n",
    "            transforms.append(fn())\n",
    "    else:\n",
    "        fn_args = fn_args.get(\"parameters\")\n",
    "        transforms.append(fn(**fn_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreprocessPipeline()"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_obj(config.preprocess, transform_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.signal_augments is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.optimizer.get(\"parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preemphasis\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtransforms\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtransform_module\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m init_transforms(config\u001b[39m.\u001b[39;49mtransforms, transforms)\n",
      "Cell \u001b[0;32mIn[33], line 10\u001b[0m, in \u001b[0;36minit_transforms\u001b[0;34m(config, module_name)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(transform)\n\u001b[1;32m      9\u001b[0m     fn \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(transform_module, transform)\n\u001b[0;32m---> 10\u001b[0m     transforms\u001b[39m.\u001b[39mappend(fn(config[transform][\u001b[39m\"\u001b[39;49m\u001b[39mparameters\u001b[39;49m\u001b[39m\"\u001b[39;49m]))\n\u001b[1;32m     12\u001b[0m \u001b[39mreturn\u001b[39;00m Compose(transforms)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import transforms as transform_module\n",
    "init_transforms(config.transforms, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import init_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.RNN(128, 64, 3, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = init_obj(config.optimizer, optim, model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unflatten_dot(dictionary):\n",
    "    resultDict = dict()\n",
    "    for key, value in dictionary.items():\n",
    "        parts = key.split(\".\")\n",
    "        d = resultDict\n",
    "        for part in parts[:-1]:\n",
    "            if part not in d:\n",
    "                d[part] = dict()\n",
    "            d = d[part]\n",
    "        d[parts[-1]] = value\n",
    "    return resultDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m config \u001b[39m=\u001b[39m unflatten_dot(\u001b[39m\"\u001b[39;49m\u001b[39m../sweep_config.yaml\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[30], line 3\u001b[0m, in \u001b[0;36munflatten_dot\u001b[0;34m(dictionary)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munflatten_dot\u001b[39m(dictionary):\n\u001b[1;32m      2\u001b[0m     resultDict \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[0;32m----> 3\u001b[0m     \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m dictionary\u001b[39m.\u001b[39;49mitems():\n\u001b[1;32m      4\u001b[0m         parts \u001b[39m=\u001b[39m key\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m         d \u001b[39m=\u001b[39m resultDict\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "config = unflatten_dot(\"../sweep_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_dir': './data/raw/IRMAS_Training_Data', 'valid_dir': './data/raw/IRMAS_Validation_Data', 'preprocess': {'PreprocessPipeline': {'target_sr': 22050}}, 'transforms': {'Preemphasis': None, 'Spectrogram': {'sample_rate': 22050, 'n_mels': 128, 'hop_length': 160, 'n_fft': 800}, 'LogTransform': None, 'PadCutToLength': {'max_length': 1024}, 'Normalize': {'mean': -4.2677393, 'std': 9.1379948}}, 'signal_augments': None, 'spec_augments': None, 'batch_size': 8, 'model_name': 'MIT/ast-finetuned-audioset-10-10-0.4593', 'loss': {'FocalLoss': {'alpha': 0.25, 'gamma': 2}}, 'optimizer': {'AdamW': {'weight_decay': 0.0001}}, 'learning_rates': {'base_model': {'lr': 1e-08}, 'classifier': {'lr': 0.001}}, 'num_accum': 2, 'verbose': True, 'EPOCHS': 4, 'metrics': ['hamming_score', 'zero_one_score', 'mean_average_precision', 'mean_f1_score'], 'scheduler': {'OneCycleLR': {'pct_start': 0.3, 'anneal_strategy': 'cos'}}, 'early_stopping': {'patience': 3, 'min_delta': 0.0001}, 'save_best_model': True}\n"
     ]
    }
   ],
   "source": [
    "CONFIG_PATH = \"../config.yaml\"\n",
    "\n",
    "with open(CONFIG_PATH) as file:\n",
    "  config = yaml.safe_load(file)\n",
    "  \n",
    "  print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"transforms\"][\"LogTransform\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_dir': {'value': './data/processed/all_sync/IRMAS_Training_Data'},\n",
       " 'valid_dir': {'value': './data/raw/IRMAS_Validation_Data'},\n",
       " 'preprocess': {'PreprocessPipeline': {'parameters': {'target_sr': {'value': 22050}}}},\n",
       " 'transforms': {'Preemphasis': {'value': None},\n",
       "  'Spectrogram': {'parameters': {'sample_rate': {'value': 22050},\n",
       "    'n_mels': {'values': [128, 96, 64]},\n",
       "    'hop_length': {'values': [64, 128, 256, 512]},\n",
       "    'n_fft': {'values': [512, 1024, 2048]}}},\n",
       "  'LogTransform': {'value': None},\n",
       "  'PadCutToLength': {'parameters': {'max_length': {'value': 1024}}},\n",
       "  'Normalize': {'parameters': {'mean': {'value': -4.2677393},\n",
       "    'std': {'value': 9.1379948}}}},\n",
       " 'batch_size': {'value': 8},\n",
       " 'loss': {'FocalLoss': {'parameters': {'alpha': {'values': [-1,\n",
       "      0.25,\n",
       "      0.5,\n",
       "      0.75]},\n",
       "    'gamma': {'values': [1, 2, 3]}}}},\n",
       " 'optimizer': {'AdamW': {'parameters': {'weight_decay': {'distribution': 'log_uniform_values',\n",
       "     'min': 1e-07,\n",
       "     'max': 0.1}}}},\n",
       " 'learning_rates': {'base_model': {'parameters': {'lr': {'distribution': 'log_uniform_values',\n",
       "     'min': 1e-08,\n",
       "     'max': 1e-05}}},\n",
       "  'classifier': {'parameters': {'lr': {'distribution': 'log_uniform_values',\n",
       "     'min': 0.0001,\n",
       "     'max': 0.1}}}},\n",
       " 'num_accum': {'values': [4, 6, 8]},\n",
       " 'EPOCHS': {'distribution': 'int_uniform', 'min': 1, 'max': 7},\n",
       " 'scheduler': {'OneCycleLR': {'parameters': {'pct_start': {'distribution': 'uniform',\n",
       "     'min': 0,\n",
       "     'max': 0.5},\n",
       "    'anneal_strategy': {'value': 'cos'}}}},\n",
       " 'early_stopping': {'parameters': {'patience': {'value': 3},\n",
       "   'min_delta': {'value': 0.0001}}},\n",
       " 'save_best_model': {'value': True},\n",
       " 'verbose': {'value': True},\n",
       " 'metrics': {'value': ['hamming_score',\n",
       "   'zero_one_score',\n",
       "   'mean_average_precision',\n",
       "   'mean_f1_score']}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unflatten_dot(config[\"parameters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
