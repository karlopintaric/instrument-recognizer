{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ASTModel, ASTConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lumen_irmas.modeling.models import StudentAST, ASTPretrained\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 were not used when initializing ASTModel: ['classifier.dense.bias', 'classifier.layernorm.weight', 'classifier.layernorm.bias', 'classifier.dense.weight']\n",
      "- This IS expected if you are initializing ASTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ASTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "teacher = ASTPretrained(11)\n",
    "student = StudentAST(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 1, 384]),\n",
       " torch.Size([1, 1, 384]),\n",
       " torch.Size([1, 1214, 384]),\n",
       " torch.Size([384, 1, 16, 16]),\n",
       " torch.Size([384])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p.shape for p in student.base_model.embeddings.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1, 768), (1, 1, 768), (1, 1214, 768), (768, 1, 16, 16), (768,)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tuple(p.shape) for p in teacher.base_model.embeddings.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(1,1,128).squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_shapes = [np.array(param.shape) for param in teacher.base_model.parameters()]\n",
    "model2_shapes = [np.array(param.shape) for param in student.base_model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  1,   1, 768]),\n",
       " array([  1,   1, 768]),\n",
       " array([   1, 1214,  768]),\n",
       " array([768,   1,  16,  16]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([3072,  768]),\n",
       " array([3072]),\n",
       " array([ 768, 3072]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([3072,  768]),\n",
       " array([3072]),\n",
       " array([ 768, 3072]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([3072,  768]),\n",
       " array([3072]),\n",
       " array([ 768, 3072]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([3072,  768]),\n",
       " array([3072]),\n",
       " array([ 768, 3072]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([3072,  768]),\n",
       " array([3072]),\n",
       " array([ 768, 3072]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([3072,  768]),\n",
       " array([3072]),\n",
       " array([ 768, 3072]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([3072,  768]),\n",
       " array([3072]),\n",
       " array([ 768, 3072]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([3072,  768]),\n",
       " array([3072]),\n",
       " array([ 768, 3072]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([3072,  768]),\n",
       " array([3072]),\n",
       " array([ 768, 3072]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([3072,  768]),\n",
       " array([3072]),\n",
       " array([ 768, 3072]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([3072,  768]),\n",
       " array([3072]),\n",
       " array([ 768, 3072]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([768, 768]),\n",
       " array([768]),\n",
       " array([3072,  768]),\n",
       " array([3072]),\n",
       " array([ 768, 3072]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768]),\n",
       " array([768])]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_params = {}\n",
    "\n",
    "# Iterate over the parameters in the first model\n",
    "for i, (name, param) in enumerate(teacher.base_model.named_parameters()):\n",
    "    # Scale the parameter using interpolate if its shape is different from that of the second model\n",
    "    target_param = student.base_model.state_dict()[name]\n",
    "    if param.shape != target_param.shape:\n",
    "        \n",
    "        squeeze_count = 0\n",
    "        permuted = False\n",
    "        while param.ndim < 4:\n",
    "            param = param.unsqueeze(0)\n",
    "            squeeze_count += 1\n",
    "        \n",
    "        if param.shape[0] > 1:\n",
    "            param = param.permute(1,2,3,0)\n",
    "            target_param = target_param.permute(1,2,3,0)\n",
    "            permuted = True\n",
    "            \n",
    "        if target_param.ndim < 2:\n",
    "            target_param = target_param.unsqueeze(0)\n",
    "        \n",
    "        scaled_param = torch.nn.functional.interpolate(param, size=(target_param.shape[-2:]), mode=\"bilinear\")\n",
    "        \n",
    "        while squeeze_count > 0:\n",
    "            scaled_param = scaled_param.squeeze(0)\n",
    "            squeeze_count -= 1\n",
    "        \n",
    "        if permuted:\n",
    "            scaled_param = scaled_param.permute(-1,0,1,2)\n",
    "        \n",
    "    else:\n",
    "        scaled_param = param\n",
    "    new_params[name] = scaled_param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student.base_model.load_state_dict(new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 1, 16, 16])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1, 384])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_param.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 384])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student.base_model.state_dict()[name].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 16, 384])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.interpolate(torch.rand(1,16,16,768), size=(16,384), mode=\"bilinear\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lumen_irmas.modeling.models import Ensemble, StudentAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = Ensemble([StudentAST(11), StudentAST(11)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = ensemble.to(\"meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
